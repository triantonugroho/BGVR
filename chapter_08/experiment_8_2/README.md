## 8.2. Data Structures for Variant Representation

### experiment_8_2

This Rust program exemplifies the languageâ€™s strength in high-performance bioinformatics workflows, particularly in pangenome variant analysis. It efficiently reads and compares VCF files using parallel set operations, calculates variant overlap statistics between cohorts, and integrates CSV-based genotype quality data analysis with export capabilities to modern formats like Parquet. The use of Rustâ€™s safe concurrency via Rayon and expressive data handling with Polars highlights a practical, scalable approach for genomic data analysis pipelines.

The current implementation strengthens robustness and scalability by leveraging comprehensive error handling through the anyhow crate, structured concurrency with Rayon, and efficient data processing using Polars. It supports parallel reading and comparison of large VCF datasets, detailed variant set algebra (including Jaccard index computation), and flexible export of summary statistics to formats like Parquet. The modular structure supports easy integration with CSV data sources, and the code handles edge cases such as malformed records and missing fields with resilience. While not yet integrated with advanced backends like GBWT or TileDB, the architecture is clean and performant, making it an ideal foundation for pangenome workflows in modern bioinformatics pipelines. Its efficient use of system resources and memory-safe parallelism also positions it well for deployment in containerized and cloud-native environments.

Over the next five years, pangenome variant analysis is expected to evolve in several key directions. First, graph-based variant representations will be paired with vector embeddings and ANN indexing (e.g., HNSW) to enable similarity search in LLM-augmented pipelines. Second, privacy-preserving computation using homomorphic encryption and secure PBWT will allow genotype-aware queries without revealing sensitive data. Third, real-time variant calling at the edgeâ€”driven by compact, pangenome-aware neural models and optimized haplotype indices like GBWTâ€”will become viable on portable devices with limited memory. These trends reinforce a shift toward integrated analysis stacks where a graph pangenome, compressed haplotype index, and columnar genotype store work seamlessly. Rust, with its safety guarantees and performance profile, is well-suited to orchestrate this stack in research and production environments alike.

#### Project Structure:

```plaintext
experiment_8_2/
â”œâ”€â”€ Cargo.toml                  # Rust dependencies
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs                 # Rust implementation
â”‚   â”œâ”€â”€ cohort_A.vcf            # Cohort A VCF file (input file)
â”‚   â”œâ”€â”€ cohort_B.vcf            # Cohort B VCF file (input file)
â”‚   â”œâ”€â”€ synthetic_variant_data.csv  # Synthetic variant data CSV file
â”‚   â”œâ”€â”€ query_results.parquet   # Query results Parquet file (output file)
â”‚   â””â”€â”€ output.txt              # Text file output
```

#### Cargo.toml

```toml
[package]
name = "pangenome_tool"
version = "0.1.0"
edition = "2024"

[dependencies]
noodles = { version = "0.6", features = ["vcf"] }  # Use noodles version 0.6 for VCF
csv = "1.1"  # for CSV file processing
serde = { version = "1.0", features = ["derive"] }  # for serialization
serde_json = "1.0"  # if you're using JSON as well
rayon = "1.5"  # for parallel processing
polars = { version = "0.47.0", features = ["parquet", "csv"] }  # For DataFrame manipulation, with CSV and Parquet features
anyhow = "1.0"  # For error handling
bio = "0.38.0"  # Ensure this version supports VCF functionality
log = "0.4"  # For logging
env_logger = "0.9"  # For logger initialization
num_cpus = "1.14.0"  # For getting CPU count
```

#### How to run:

run main.rs in wsl:

```wsl
cargo run | tee output.txt
```

(run main.rs with cohort_A.vcf, cohort_B.vcf and synthetic_variant_data.csv as input files and create query_results.parquet output file)


#### ğŸ“‹ Explanation of the Output
##### âœ… Parallel Processing and File Reading
The command:

```bash
cargo run | tee output.txt
```

executes your Rust application and logs its output to output.txt.

* Parallelism: The output shows that the program starts with 8 threads, utilizing all available CPU cores via the rayon thread pool:

```text
Starting pangenome analysis with 8 threads
```

* VCF Reading:

```text
Reading variants from cohort_A.vcf
Reading variants from cohort_B.vcf
```

Each file was parsed in under 10 milliseconds:

```text
Read 1000 variants from cohort_A.vcf in 5.77ms
Read 1000 variants from cohort_B.vcf in 6.09ms
```

##### ğŸ§¬ Variant Set Algebra Results

* Union (A âˆª B): 2000 variants â€” all unique across both cohorts.

* Intersection (A âˆ© B): 0 â€” no shared variants between the two sets.

* A \ B and B \ A: 1000 variants each â€” all variants are cohort-specific.

* Jaccard Index: 0.0000, indicating no overlap between cohort A and B variants.

```text
Variant set comparison:
  AâˆªB = 2000 variants
  Aâˆ©B = 0 variants
  A\B = 1000 variants
  B\A = 1000 variants
  Jaccard index = 0.0000
```

##### ğŸ“Š CSV File Processing

* synthetic_variant_data.csv was read into a Polars DataFrame with:
  * 1000 rows
  * 7 fields: CHROM, POS, REF, ALT, GT, GQ, and DP
```text
DataFrame query results:
  Variants in CSV: 1000
  DataFrame schema: ...
```

* Statistics Note: Skipped due to compatibility issues with polars 0.47.0.

##### ğŸ’¾ Export Operation

* Successfully exported the DataFrame to a Parquet file:

```text
Exported DataFrame with 1000 rows to query_results.parquet
```

##### â±ï¸ Performance

* Total runtime: Only 55 milliseconds for the entire process, which includes:
  * Multi-threaded parsing of 2,000 VCF entries
  * Set operations
  * Reading a 1,000-row CSV
  * Exporting to Parquet

```text
Total execution time: 55.27ms
```

#### âœ… Conclusion
This execution confirms that your Rust-based pangenome tool is:

* âš¡ Fast and Efficient: Processes multiple files and performs variant set algebra and CSV I/O in under 60 milliseconds.

* ğŸ§µ Scalable: Uses all available cores with rayon to parallelize I/O-bound and compute-bound tasks.

* ğŸ› ï¸ Reliable: Provides error handling, schema validation, and gracefully skips unsupported features (e.g., Polars stats).

* ğŸ§¬ Biologically Informative: Set operations reveal that the two cohorts have zero shared variants, likely indicating they originate from entirely distinct populations or datasets.

* ğŸ§± Ready for Integration: Generates intermediate results in efficient formats like Parquet, enabling easy downstream analysis in cloud-native or Python-based workflows.
